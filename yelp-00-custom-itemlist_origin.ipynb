{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Data Block Nirvana (a journey through the fastai data block API)\n",
    "\n",
    "This notebook illustrates how to create a custom `ItemList` for use in the fastai data block API.  It is heavily annotated to further aid in also understanding how all the different bits in the API interact as well as what is happening at each step and why.\n",
    "\n",
    "Please consult the [fastai docs](https://docs.fast.ai/) for installing required packages and setting up your environment to run the code below.\n",
    "\n",
    "The accompanying Medium article highlighing the data block API mechanics based on my work here can be found [here](https://medium.com/@wgilliam/finding-data-block-nirvana-a-journey-through-the-fastai-data-block-api-c38210537fe4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp Dataset\n",
    "\n",
    "This example utilize a subset of the Yelp review dataset I've made available as part of the code repo for the purposes of illustrating how my `MixedTabularList` would work with a pandas DataFrame containing categorical, continuous, and numercalized text data.  The full dataset and documentation can be found following the links below.\n",
    "\n",
    "Available from https://www.yelp.com/dataset/download  \n",
    "Documentation here:  https://www.yelp.com/dataset/documentation/main  \n",
    "More information here:  https://www.yelp.com/dataset\n",
    "\n",
    "Unzip the `joined_sample.zip` .csv file into a `data/yelp_dataset` folder relative to this notebook and you should be good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastai version: 1.0.39\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "\n",
    "from fastai.tabular import *\n",
    "from fastai.text import *\n",
    "from fastai.text.data import _join_texts\n",
    "\n",
    "print(f'fastai version: {__version__}')  #=> I test this against 1.0.39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU: 1\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(1)\n",
    "print(f'using GPU: {torch.cuda.current_device()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=Path('data/yelp_dataset/')\n",
    "# PATH.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ItemBase subclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ItemBase` defines the inputs for your custom dataset, the X and optionally y values you are going to feed into the `forward` function of your pytorch model.  Here we define what an an input item looks like (we'll let fastai infer the `ItemBase` type to use based on our target values).\n",
    "\n",
    "If your custom `ItemBase` needs to have some kind of data augmentation applied to it, you should overload the `apply_tfms` method as needed.  This method will be called you apply a `transform` block via the Data Block API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedTabularLine(TabularLine):\n",
    "    \"Item's that include both tabular data(`conts` and `cats`) and textual data (numericalized `ids`)\"\n",
    "    \n",
    "    def __init__(self, cats, conts, cat_classes, col_names, txt_ids, txt_cols, txt_string):\n",
    "        # tabular\n",
    "        super().__init__(cats, conts, cat_classes, col_names)\n",
    "\n",
    "        # add the text bits\n",
    "        self.text_ids = txt_ids\n",
    "        self.text_cols = txt_cols\n",
    "        self.text = txt_string\n",
    "        \n",
    "        # append numericalted text data to your input (represents your X values that are fed into your model)\n",
    "        # self.data = [tensor(cats), tensor(conts), tensor(txt_ids)]\n",
    "        self.data += [ np.array(txt_ids, dtype=np.int64) ]\n",
    "        self.obj = self.data\n",
    "        \n",
    "    def __str__(self):\n",
    "        res = super().__str__() + f'Text: {self.text}'\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define custom Processor, DataBunch, and utility methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our custom `ItemList` is going to require a custom `PreProcessor` and a custom `DataBunch`, so we define them here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedTabularProcessor(TabularProcessor):\n",
    "    \n",
    "    def __init__(self, ds:ItemList=None, procs=None, \n",
    "                 tokenizer:Tokenizer=None, chunksize:int=10000,\n",
    "                 vocab:Vocab=None, max_vocab:int=60000, min_freq:int=2):\n",
    "        #pdb.set_trace()\n",
    "        super().__init__(ds, procs)\n",
    "    \n",
    "        self.tokenizer, self.chunksize = ifnone(tokenizer, Tokenizer()), chunksize\n",
    "        \n",
    "        vocab = ifnone(vocab, ds.vocab if ds is not None else None)\n",
    "        self.vocab, self.max_vocab, self.min_freq = vocab, max_vocab, min_freq\n",
    "        \n",
    "    # process a single item in a dataset\n",
    "    # NOTE: THIS IS METHOD HAS NOT BEEN TESTED AT THIS POINT (WILL COVER IN A FUTURE ARTICLE)\n",
    "    def process_one(self, item):\n",
    "        # process tabular data (copied form tabular.data)\n",
    "        df = pd.DataFrame([item, item])\n",
    "        for proc in self.procs: proc(df, test=True)\n",
    "            \n",
    "        if len(self.cat_names) != 0:\n",
    "            codes = np.stack([c.cat.codes.values for n,c in df[self.cat_names].items()], 1).astype(np.int64) + 1\n",
    "        else: \n",
    "            codes = [[]]\n",
    "            \n",
    "        if len(self.cont_names) != 0:\n",
    "            conts = np.stack([c.astype('float32').values for n,c in df[self.cont_names].items()], 1)\n",
    "        else: \n",
    "            conts = [[]]\n",
    "            \n",
    "        classes = None\n",
    "        col_names = list(df[self.cat_names].columns.values) + list(df[self.cont_names].columns.values)\n",
    "        \n",
    "        # process textual data\n",
    "        if len(self.text_cols) != 0:\n",
    "            txt = _join_texts(df[self.text_cols].values, (len(self.text_cols) > 1))\n",
    "            txt_toks = self.tokenizer._process_all_1(txt)[0]\n",
    "            text_ids = np.array(self.vocab.numericalize(txt_toks), dtype=np.int64)\n",
    "        else:\n",
    "            txt_toks, text_ids = None, [[]]\n",
    "            \n",
    "        # return ItemBase\n",
    "        return MixedTabularLine(codes[0], conts[0], classes, col_names, text_ids, self.txt_cols, txt_toks)\n",
    "    \n",
    "    # processes the entire dataset\n",
    "    def process(self, ds):\n",
    "        #pdb.set_trace()\n",
    "        # process tabular data and then set \"preprocessed=False\" since we still have text data possibly\n",
    "        super().process(ds)\n",
    "        ds.preprocessed = False\n",
    "        \n",
    "        # process text data from column(s) containing text\n",
    "        if len(ds.text_cols) != 0:\n",
    "            texts = _join_texts(ds.xtra[ds.text_cols].values, (len(ds.text_cols) > 1))\n",
    "\n",
    "            # tokenize (set = .text)\n",
    "            tokens = []\n",
    "            for i in progress_bar(range(0, len(ds), self.chunksize), leave=False):\n",
    "                tokens += self.tokenizer.process_all(texts[i:i+self.chunksize])\n",
    "            ds.text = tokens\n",
    "\n",
    "            # set/build vocab\n",
    "            if self.vocab is None: self.vocab = Vocab.create(ds.text, self.max_vocab, self.min_freq)\n",
    "            ds.vocab = self.vocab\n",
    "            ds.text_ids = [ np.array(self.vocab.numericalize(toks), dtype=np.int64) for toks in ds.text ]\n",
    "        else:\n",
    "            ds.text, ds.vocab, ds.text_ids = None, None, []\n",
    "            \n",
    "        ds.preprocessed = True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to the \"fasta.text.data.pad_collate\" except that it is designed to work with MixedTabularLine items,\n",
    "# where the final thing in an item is the numericalized text ids.\n",
    "# we need a collate function to ensure a square matrix with the text ids, which will be of variable length.\n",
    "def mixed_tabular_pad_collate(samples:BatchSamples, \n",
    "                              pad_idx:int=1, pad_first:bool=True) -> Tuple[LongTensor, LongTensor]:\n",
    "    \"Function that collect samples and adds padding.\"\n",
    "\n",
    "    samples = to_data(samples)\n",
    "    max_len = max([len(s[0][-1]) for s in samples])\n",
    "    res = torch.zeros(len(samples), max_len).long() + pad_idx\n",
    "   \n",
    "    for i,s in enumerate(samples):\n",
    "        if pad_first: \n",
    "            res[i,-len(s[0][-1]):] = LongTensor(s[0][-1])\n",
    "        else:         \n",
    "            res[i,:len(s[0][-1]):] = LongTensor(s[0][-1])\n",
    "            \n",
    "        # replace the text_ids array (the last thing in the inputs) with the padded tensor matrix\n",
    "        s[0][-1] = res[i]\n",
    "              \n",
    "    # for the inputs, return a list containing 3 elements: a list of cats, a list of conts, and a list of text_ids\n",
    "    return [x for x in zip(*[s[0] for s in samples])], tensor([s[1] for s in samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each \"ds\" is of type LabelList(Dataset)\n",
    "class MixedTabularDataBunch(DataBunch):\n",
    "    @classmethod\n",
    "    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', bs=64, \n",
    "               pad_idx=1, pad_first=True, no_check:bool=False, **kwargs) -> DataBunch:\n",
    "        \n",
    "        # only thing we're doing here is setting the collate_fn = to our new \"pad_collate\" method above\n",
    "        collate_fn = partial(mixed_tabular_pad_collate, pad_idx=pad_idx, pad_first=pad_first)\n",
    "        \n",
    "        return super().create(train_ds, valid_ds, test_ds, path=path, bs=bs, num_workers=1,\n",
    "                              collate_fn=collate_fn, no_check=no_check, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ItemList subclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `ItemList` consists of a set of `ItemBase` objects. Once created, you can use any of splitting or labeling methods prior to creating a `DataBunch` for training.\n",
    "\n",
    "You'll likely want to set the following three class variables to something specific to your situation:\n",
    "\n",
    "**`_bunch`**:  \n",
    "The name of the class used to create a `DataBunch`.  `TabularList` uses the default `DataBunch` as is and so does not set this variable. We create a custom `DataBunch` here because we need to add padding to the column with the text ids in order to ensure a square matrix per batch before integrating the text bits with the tabular.\n",
    "\n",
    "When you call `databunch()` via the Data Block API, `_bunch.create` will be called passing in the datasets (training, validation and optionally test) defined by your `ItemLists` and returning a set of `DataLoader`s in a `DataBunch` for training.\n",
    "\n",
    "**`_processor`**:  \n",
    "A class or list of classes of type `PreProcessor` that will be used to create the default processor for this `ItemList`.\n",
    "\n",
    "The processors are **called at the end of the labelling** to apply some kind of function on your items. The **default processor of the inputs** can be overriden by passing a `processor` in the kwargs when creating the `ItemList`, the **default processor of the targets** can be overriden by passing a `processor` in the kwargs of the labelling function.\n",
    "\n",
    "Processors are useful for pre-processing data, and **you also need to save any computed state required for future datasets when `data.export()` is called.**\n",
    "\n",
    "**`_item_cls`**:   \n",
    "The name of the class that will be used to create the \"items\" by default.\n",
    "\n",
    "**`_label_cls`**:   \n",
    "The name of the class that will be used to create the labels by default. (**If this variable is set to None, the label class will be guessed** between `CategoryList`, `MultiCategoryList` and `FloatList` depending on the type of the first item. Since we are creating a custom `ItemList` with a very distinct signature, we want to set it to that class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedTabularList(TabularList):\n",
    "    \"A custom `ItemList` that merges tabular data along with textual data\"\n",
    "    \n",
    "    _item_cls = MixedTabularLine\n",
    "    _processor = MixedTabularProcessor\n",
    "    _bunch = MixedTabularDataBunch\n",
    "    \n",
    "    def __init__(self, items:Iterator, cat_names:OptStrList=None, cont_names:OptStrList=None, \n",
    "                 text_cols=None, vocab:Vocab=None, pad_idx:int=1, \n",
    "                 procs=None, **kwargs) -> 'MixedTabularList':\n",
    "        #pdb.set_trace()\n",
    "        super().__init__(items, cat_names, cont_names, procs, **kwargs)\n",
    "        \n",
    "        self.cols = [] if cat_names == None else cat_names.copy()\n",
    "        if cont_names: self.cols += cont_names.copy()\n",
    "        if txt_cols: self.cols += text_cols.copy()\n",
    "        \n",
    "        self.text_cols, self.vocab, self.pad_idx = text_cols, vocab, pad_idx\n",
    "        \n",
    "        # add any ItemList state into \"copy_new\" that needs to be copied each time \"new()\" is called; \n",
    "        # your ItemList acts as a prototype for training, validation, and/or test ItemList instances that\n",
    "        # are created via ItemList.new()\n",
    "        self.copy_new += ['text_cols', 'vocab', 'pad_idx']\n",
    "        \n",
    "        self.preprocessed = False\n",
    "        \n",
    "    # defines how to construct an ItemBase from the data in the ItemList.items array\n",
    "    def get(self, i):\n",
    "        if not self.preprocessed: \n",
    "            return self.xtra.iloc[i][self.cols] if hasattr(self, 'xtra') else self.items[i]\n",
    "        \n",
    "        codes = [] if self.codes is None else self.codes[i]\n",
    "        conts = [] if self.conts is None else self.conts[i]\n",
    "        text_ids = [] if self.text_ids is None else self.text_ids[i]\n",
    "        text_string = None if self.text_ids is None else self.vocab.textify(self.text_ids[i])\n",
    "        \n",
    "        return self._item_cls(codes, conts, self.classes, self.col_names, text_ids, self.text_cols, text_string)\n",
    "    \n",
    "    # this is the method that is called in data.show_batch(), learn.predict() or learn.show_results() \n",
    "    # to transform a pytorch tensor back in an ItemBase. \n",
    "    # in a way, it does the opposite of calling ItemBase.data. It should take a tensor t and return \n",
    "    # the same king of thing as the get method.\n",
    "    def reconstruct(self, t:Tensor):\n",
    "        return self._item_cls(t[0], t[1], self.classes, self.col_names, \n",
    "                              t[2], self.text_cols, self.vocab.textify(t[2]))\n",
    "    \n",
    "    # tells fastai how to display a custom ItemBase when data.show_batch() is called\n",
    "    def show_xys(self, xs, ys) -> None:\n",
    "        \"Show the `xs` (inputs) and `ys` (targets).\"\n",
    "        from IPython.display import display, HTML\n",
    "        \n",
    "        # show tabular\n",
    "        display(HTML('TABULAR:<br>'))\n",
    "        super().show_xys(xs, ys)\n",
    "        \n",
    "        # show text\n",
    "        items = [['text_data', 'target']]\n",
    "        for i, (x,y) in enumerate(zip(xs,ys)):\n",
    "            res = []\n",
    "            res += [' '.join([ f'{tok}({self.vocab.stoi[tok]})' \n",
    "                              for tok in x.text.split() if (not self.vocab.stoi[tok] == self.pad_idx) ])]\n",
    "                \n",
    "            res += [str(y)]\n",
    "            items.append(res)\n",
    "            \n",
    "        col_widths = [90, 1]\n",
    "        \n",
    "        display(HTML('TEXT:<br>'))\n",
    "        display(HTML(text2html_table(items, (col_widths))))\n",
    "        \n",
    "    # tells fastai how to display a custom ItemBase when learn.show_results() is called\n",
    "    def show_xyzs(self, xs, ys, zs):\n",
    "        \"Show `xs` (inputs), `ys` (targets) and `zs` (predictions).\"\n",
    "        from IPython.display import display, HTML\n",
    "        \n",
    "        # show tabular\n",
    "        super().show_xyzs(xs, ys, zs)\n",
    "        \n",
    "        # show text\n",
    "        items = [['text_data','target', 'prediction']]\n",
    "        for i, (x,y,z) in enumerate(zip(xs,ys,zs)):\n",
    "            res = []\n",
    "            res += [' '.join([ f'{tok}({self.vocab.stoi[tok]})'\n",
    "                              for tok in x.text.split() if (not self.vocab.stoi[tok] == self.pad_idx) ])]\n",
    "                \n",
    "            res += [str(y),str(z)]\n",
    "            items.append(res)\n",
    "            \n",
    "        col_widths = [90, 1, 1]\n",
    "        display(HTML('<br>' + text2html_table(items, (col_widths))))\n",
    "    \n",
    "        \n",
    "    @classmethod\n",
    "    def from_df(cls, df:DataFrame, cat_names:OptStrList=None, cont_names:OptStrList=None, \n",
    "                text_cols=None, vocab=None, procs=None, **kwargs) -> 'ItemList':\n",
    "        \n",
    "        return cls(items=range(len(df)), cat_names=cat_names, cont_names=cont_names, \n",
    "                   text_cols=text_cols, vocab=vocab, procs=procs, xtra=df, **kwargs)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch joined yelp reviews (includes busines and user info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_average_stars</th>\n",
       "      <th>...</th>\n",
       "      <th>business_hours</th>\n",
       "      <th>business_is_open</th>\n",
       "      <th>business_latitude</th>\n",
       "      <th>business_longitude</th>\n",
       "      <th>business_name</th>\n",
       "      <th>business_neighborhood</th>\n",
       "      <th>business_postal_code</th>\n",
       "      <th>business_review_count</th>\n",
       "      <th>business_stars</th>\n",
       "      <th>business_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8jpIK1WHmzzbXPaK51GenQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-08-08</td>\n",
       "      <td>3</td>\n",
       "      <td>W7wcVRiw5T8TMrmGnxPsxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>I've been here at least 10 times ... I like it...</td>\n",
       "      <td>1</td>\n",
       "      <td>g6gTSnUKZIxLZPQVrFKscw</td>\n",
       "      <td>4.14</td>\n",
       "      <td>...</td>\n",
       "      <td>{'Tuesday': '6:30-14:30', 'Wednesday': '6:30-1...</td>\n",
       "      <td>0</td>\n",
       "      <td>33.320994</td>\n",
       "      <td>-111.912682</td>\n",
       "      <td>Dessie's Cafe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85226</td>\n",
       "      <td>67</td>\n",
       "      <td>3.5</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wH4Q0y8C-lkq21yf4WWedw</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-31</td>\n",
       "      <td>0</td>\n",
       "      <td>emypFL3PJjQBcllPZw_d5A</td>\n",
       "      <td>5</td>\n",
       "      <td>Although I had heard of Nekter, mainly from se...</td>\n",
       "      <td>2</td>\n",
       "      <td>LAEJWZSvzsfWJ686VOaQig</td>\n",
       "      <td>5.00</td>\n",
       "      <td>...</td>\n",
       "      <td>{'Monday': '6:30-20:0', 'Tuesday': '6:30-20:0'...</td>\n",
       "      <td>1</td>\n",
       "      <td>33.580474</td>\n",
       "      <td>-111.881062</td>\n",
       "      <td>Nekter Juice Bar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85260</td>\n",
       "      <td>59</td>\n",
       "      <td>4.0</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cRMC2eQ9CP6ivhEY8EdaGg</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-09-13</td>\n",
       "      <td>0</td>\n",
       "      <td>5X5ISEAp6HFTpMd_wlq_9w</td>\n",
       "      <td>3</td>\n",
       "      <td>Last week I met up with a highschool friend fo...</td>\n",
       "      <td>1</td>\n",
       "      <td>TwilnpgwW43r9-O2AS4PDQ</td>\n",
       "      <td>3.14</td>\n",
       "      <td>...</td>\n",
       "      <td>{'Monday': '12:0-21:0', 'Tuesday': '12:0-21:0'...</td>\n",
       "      <td>0</td>\n",
       "      <td>43.664193</td>\n",
       "      <td>-79.380196</td>\n",
       "      <td>Chino Locos</td>\n",
       "      <td>Church-Wellesley Village</td>\n",
       "      <td>M4Y 2C5</td>\n",
       "      <td>34</td>\n",
       "      <td>3.5</td>\n",
       "      <td>ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zunMkZ4U2eVojempQtLngg</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-03-07</td>\n",
       "      <td>0</td>\n",
       "      <td>OGekU1U_wWgV--zL2gEgYw</td>\n",
       "      <td>4</td>\n",
       "      <td>A friend and I were driving by and decided to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>eITkQlKYsYqOBASP-QS0iQ</td>\n",
       "      <td>3.72</td>\n",
       "      <td>...</td>\n",
       "      <td>{'Monday': '11:0-1:0', 'Tuesday': '11:0-1:0', ...</td>\n",
       "      <td>0</td>\n",
       "      <td>33.639158</td>\n",
       "      <td>-112.185110</td>\n",
       "      <td>The Australian AZ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85308</td>\n",
       "      <td>26</td>\n",
       "      <td>2.5</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1vLf-v7foAu3tJ7vAEoKdA</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-11-26</td>\n",
       "      <td>1</td>\n",
       "      <td>tTe2cLFmpkLop3wKcT0Zgw</td>\n",
       "      <td>5</td>\n",
       "      <td>Our Bulldog LOVES this place and so do we! Won...</td>\n",
       "      <td>0</td>\n",
       "      <td>l3okl_UjyNdqRKAzYGdWaA</td>\n",
       "      <td>2.95</td>\n",
       "      <td>...</td>\n",
       "      <td>{'Monday': '7:30-19:0', 'Tuesday': '7:30-19:0'...</td>\n",
       "      <td>1</td>\n",
       "      <td>33.582848</td>\n",
       "      <td>-111.929296</td>\n",
       "      <td>Lori's Grooming</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85254</td>\n",
       "      <td>148</td>\n",
       "      <td>5.0</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool        date  funny               review_id  \\\n",
       "0  8jpIK1WHmzzbXPaK51GenQ     1  2012-08-08      3  W7wcVRiw5T8TMrmGnxPsxQ   \n",
       "1  wH4Q0y8C-lkq21yf4WWedw     0  2015-01-31      0  emypFL3PJjQBcllPZw_d5A   \n",
       "2  cRMC2eQ9CP6ivhEY8EdaGg     1  2010-09-13      0  5X5ISEAp6HFTpMd_wlq_9w   \n",
       "3  zunMkZ4U2eVojempQtLngg     1  2014-03-07      0  OGekU1U_wWgV--zL2gEgYw   \n",
       "4  1vLf-v7foAu3tJ7vAEoKdA     0  2014-11-26      1  tTe2cLFmpkLop3wKcT0Zgw   \n",
       "\n",
       "   stars                                               text  useful  \\\n",
       "0      4  I've been here at least 10 times ... I like it...       1   \n",
       "1      5  Although I had heard of Nekter, mainly from se...       2   \n",
       "2      3  Last week I met up with a highschool friend fo...       1   \n",
       "3      4  A friend and I were driving by and decided to ...       1   \n",
       "4      5  Our Bulldog LOVES this place and so do we! Won...       0   \n",
       "\n",
       "                  user_id  user_average_stars       ...        \\\n",
       "0  g6gTSnUKZIxLZPQVrFKscw                4.14       ...         \n",
       "1  LAEJWZSvzsfWJ686VOaQig                5.00       ...         \n",
       "2  TwilnpgwW43r9-O2AS4PDQ                3.14       ...         \n",
       "3  eITkQlKYsYqOBASP-QS0iQ                3.72       ...         \n",
       "4  l3okl_UjyNdqRKAzYGdWaA                2.95       ...         \n",
       "\n",
       "                                      business_hours  business_is_open  \\\n",
       "0  {'Tuesday': '6:30-14:30', 'Wednesday': '6:30-1...                 0   \n",
       "1  {'Monday': '6:30-20:0', 'Tuesday': '6:30-20:0'...                 1   \n",
       "2  {'Monday': '12:0-21:0', 'Tuesday': '12:0-21:0'...                 0   \n",
       "3  {'Monday': '11:0-1:0', 'Tuesday': '11:0-1:0', ...                 0   \n",
       "4  {'Monday': '7:30-19:0', 'Tuesday': '7:30-19:0'...                 1   \n",
       "\n",
       "   business_latitude  business_longitude      business_name  \\\n",
       "0          33.320994         -111.912682      Dessie's Cafe   \n",
       "1          33.580474         -111.881062   Nekter Juice Bar   \n",
       "2          43.664193          -79.380196        Chino Locos   \n",
       "3          33.639158         -112.185110  The Australian AZ   \n",
       "4          33.582848         -111.929296    Lori's Grooming   \n",
       "\n",
       "      business_neighborhood  business_postal_code  business_review_count  \\\n",
       "0                       NaN                 85226                     67   \n",
       "1                       NaN                 85260                     59   \n",
       "2  Church-Wellesley Village               M4Y 2C5                     34   \n",
       "3                       NaN                 85308                     26   \n",
       "4                       NaN                 85254                    148   \n",
       "\n",
       "   business_stars  business_state  \n",
       "0             3.5              AZ  \n",
       "1             4.0              AZ  \n",
       "2             3.5              ON  \n",
       "3             2.5              AZ  \n",
       "4             5.0              AZ  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cool</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>0.559900</td>\n",
       "      <td>2.017199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funny</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>0.476320</td>\n",
       "      <td>2.406208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>388.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>3.733740</td>\n",
       "      <td>1.452036</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>useful</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>1.345560</td>\n",
       "      <td>3.286281</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>212.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_average_stars</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>3.740348</td>\n",
       "      <td>0.797706</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>3.810000</td>\n",
       "      <td>4.210000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_compliment_cool</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>33.420340</td>\n",
       "      <td>267.086374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>13014.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_compliment_cute</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>1.572800</td>\n",
       "      <td>28.521646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2250.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_compliment_funny</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>33.420340</td>\n",
       "      <td>267.086374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>13014.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_compliment_hot</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>23.139540</td>\n",
       "      <td>225.898754</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12390.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_compliment_list</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>0.987660</td>\n",
       "      <td>20.821018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2259.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_compliment_more</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>2.853180</td>\n",
       "      <td>32.303978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3574.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_compliment_note</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>15.070380</td>\n",
       "      <td>104.953892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4899.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_compliment_photos</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>12.852240</td>\n",
       "      <td>176.193671</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11987.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_compliment_plain</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>37.170780</td>\n",
       "      <td>297.905563</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>11741.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_compliment_profile</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>2.203580</td>\n",
       "      <td>46.838526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5659.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_compliment_writer</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>12.473600</td>\n",
       "      <td>97.059960</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5668.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_cool</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>209.271280</td>\n",
       "      <td>2086.063566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>86136.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_fans</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>11.118740</td>\n",
       "      <td>52.620700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2028.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_funny</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>140.982840</td>\n",
       "      <td>1636.335667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>83218.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_review_count</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>117.436000</td>\n",
       "      <td>330.615603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>9278.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_useful</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>260.703300</td>\n",
       "      <td>2199.603589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>91508.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business_is_open</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>0.873840</td>\n",
       "      <td>0.332033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business_latitude</th>\n",
       "      <td>49999.0</td>\n",
       "      <td>37.205874</td>\n",
       "      <td>4.141307</td>\n",
       "      <td>-34.513715</td>\n",
       "      <td>33.607472</td>\n",
       "      <td>36.105315</td>\n",
       "      <td>40.441203</td>\n",
       "      <td>59.439215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business_longitude</th>\n",
       "      <td>49999.0</td>\n",
       "      <td>-102.841327</td>\n",
       "      <td>16.140805</td>\n",
       "      <td>-123.587426</td>\n",
       "      <td>-115.154519</td>\n",
       "      <td>-111.980988</td>\n",
       "      <td>-81.057520</td>\n",
       "      <td>115.086769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business_review_count</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>373.651600</td>\n",
       "      <td>782.788750</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>346.250000</td>\n",
       "      <td>7968.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business_stars</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>3.736580</td>\n",
       "      <td>0.754123</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           count        mean          std         min  \\\n",
       "cool                     50000.0    0.559900     2.017199    0.000000   \n",
       "funny                    50000.0    0.476320     2.406208    0.000000   \n",
       "stars                    50000.0    3.733740     1.452036    1.000000   \n",
       "useful                   50000.0    1.345560     3.286281    0.000000   \n",
       "user_average_stars       50000.0    3.740348     0.797706    1.000000   \n",
       "user_compliment_cool     50000.0   33.420340   267.086374    0.000000   \n",
       "user_compliment_cute     50000.0    1.572800    28.521646    0.000000   \n",
       "user_compliment_funny    50000.0   33.420340   267.086374    0.000000   \n",
       "user_compliment_hot      50000.0   23.139540   225.898754    0.000000   \n",
       "user_compliment_list     50000.0    0.987660    20.821018    0.000000   \n",
       "user_compliment_more     50000.0    2.853180    32.303978    0.000000   \n",
       "user_compliment_note     50000.0   15.070380   104.953892    0.000000   \n",
       "user_compliment_photos   50000.0   12.852240   176.193671    0.000000   \n",
       "user_compliment_plain    50000.0   37.170780   297.905563    0.000000   \n",
       "user_compliment_profile  50000.0    2.203580    46.838526    0.000000   \n",
       "user_compliment_writer   50000.0   12.473600    97.059960    0.000000   \n",
       "user_cool                50000.0  209.271280  2086.063566    0.000000   \n",
       "user_fans                50000.0   11.118740    52.620700    0.000000   \n",
       "user_funny               50000.0  140.982840  1636.335667    0.000000   \n",
       "user_review_count        50000.0  117.436000   330.615603    0.000000   \n",
       "user_useful              50000.0  260.703300  2199.603589    0.000000   \n",
       "business_is_open         50000.0    0.873840     0.332033    0.000000   \n",
       "business_latitude        49999.0   37.205874     4.141307  -34.513715   \n",
       "business_longitude       49999.0 -102.841327    16.140805 -123.587426   \n",
       "business_review_count    50000.0  373.651600   782.788750    3.000000   \n",
       "business_stars           50000.0    3.736580     0.754123    1.000000   \n",
       "\n",
       "                                25%         50%         75%           max  \n",
       "cool                       0.000000    0.000000    1.000000     90.000000  \n",
       "funny                      0.000000    0.000000    0.000000    388.000000  \n",
       "stars                      3.000000    4.000000    5.000000      5.000000  \n",
       "useful                     0.000000    0.000000    2.000000    212.000000  \n",
       "user_average_stars         3.400000    3.810000    4.210000      5.000000  \n",
       "user_compliment_cool       0.000000    0.000000    2.000000  13014.000000  \n",
       "user_compliment_cute       0.000000    0.000000    0.000000   2250.000000  \n",
       "user_compliment_funny      0.000000    0.000000    2.000000  13014.000000  \n",
       "user_compliment_hot        0.000000    0.000000    1.000000  12390.000000  \n",
       "user_compliment_list       0.000000    0.000000    0.000000   2259.000000  \n",
       "user_compliment_more       0.000000    0.000000    1.000000   3574.000000  \n",
       "user_compliment_note       0.000000    0.000000    2.000000   4899.000000  \n",
       "user_compliment_photos     0.000000    0.000000    0.000000  11987.000000  \n",
       "user_compliment_plain      0.000000    0.000000    3.000000  11741.000000  \n",
       "user_compliment_profile    0.000000    0.000000    0.000000   5659.000000  \n",
       "user_compliment_writer     0.000000    0.000000    1.000000   5668.000000  \n",
       "user_cool                  0.000000    0.000000    4.000000  86136.000000  \n",
       "user_fans                  0.000000    0.000000    4.000000   2028.000000  \n",
       "user_funny                 0.000000    0.000000    6.000000  83218.000000  \n",
       "user_review_count          7.000000   23.000000   93.000000   9278.000000  \n",
       "user_useful                0.000000    3.000000   26.000000  91508.000000  \n",
       "business_is_open           1.000000    1.000000    1.000000      1.000000  \n",
       "business_latitude         33.607472   36.105315   40.441203     59.439215  \n",
       "business_longitude      -115.154519 -111.980988  -81.057520    115.086769  \n",
       "business_review_count     35.000000  115.000000  346.250000   7968.000000  \n",
       "business_stars             3.500000    4.000000    4.000000      5.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "joined_df = pd.read_csv(PATH/'joined_sample.csv', index_col=None)\n",
    "\n",
    "display(len(joined_df))\n",
    "display(joined_df.head())\n",
    "display(joined_df.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use and test our MixedTabularList ItemList with the Data Block API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['business_id', 'user_id', 'business_stars', 'business_postal_code', 'business_state']\n",
    "cont_cols = ['useful', 'user_average_stars', 'user_review_count', 'business_review_count']\n",
    "txt_cols = ['text']\n",
    "\n",
    "dep_var = ['stars']\n",
    "\n",
    "procs = [FillMissing, Categorify, Normalize]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Define the source of your inputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-32-1a4a4ec6164c>(12)__init__()\n",
      "-> super().__init__(items, cat_names, cont_names, procs, **kwargs)\n",
      "(Pdb) c\n"
     ]
    }
   ],
   "source": [
    "il = MixedTabularList.from_df(joined_df, cat_cols, cont_cols, txt_cols, vocab=None, procs=procs, path=PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATS:\n",
      "['business_id', 'user_id', 'business_stars', 'business_postal_code', 'business_state']\n",
      "CONTS:\n",
      "['useful', 'user_average_stars', 'user_review_count', 'business_review_count']\n",
      "TEXT COLS:\n",
      "['text']\n",
      "PROCS:\n",
      "[<class 'fastai.tabular.transform.FillMissing'>, <class 'fastai.tabular.transform.Categorify'>, <class 'fastai.tabular.transform.Normalize'>]\n",
      "\n",
      "business_id                                         8jpIK1WHmzzbXPaK51GenQ\n",
      "user_id                                             g6gTSnUKZIxLZPQVrFKscw\n",
      "business_stars                                                         3.5\n",
      "business_postal_code                                                 85226\n",
      "business_state                                                          AZ\n",
      "useful                                                                   1\n",
      "user_average_stars                                                    4.14\n",
      "user_review_count                                                       26\n",
      "business_review_count                                                   67\n",
      "text                     I've been here at least 10 times ... I like it...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(f'CATS:\\n{il.cat_names}')\n",
    "print(f'CONTS:\\n{il.cont_names}')\n",
    "print(f'TEXT COLS:\\n{il.text_cols}')\n",
    "print(f'PROCS:\\n{il.procs}')\n",
    "print('')\n",
    "print(il.get(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Split your dataset into training and validation `ItemList`s**\n",
    "\n",
    "This is going to trigger the `ItemList.new()` method getting called for each `ItemList` it needs to create (e.g., train, validation).  Here it will be called 2x, once to create the training dataset and then to create the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-32-1a4a4ec6164c>(12)__init__()\n",
      "-> super().__init__(items, cat_names, cont_names, procs, **kwargs)\n",
      "(Pdb) c\n",
      "> <ipython-input-32-1a4a4ec6164c>(12)__init__()\n",
      "-> super().__init__(items, cat_names, cont_names, procs, **kwargs)\n",
      "(Pdb) c\n"
     ]
    }
   ],
   "source": [
    "ils = il.random_split_by_pct(valid_pct=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45000, 5000, PosixPath('data/yelp_dataset'))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ils.train), len(ils.valid), ils.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Add your labels (your targets or \"y\" values)**\n",
    "\n",
    "This will grab the targets (the \"y\") for each `ItemList` in your `ItemLists` object (e.g, `.train`, `.valid`) and build a `LabelList(Dataset)` for each accordingly that is then combined in and returned in a `LabelLists` object.\n",
    "\n",
    "You'll notice that the processor is created 1x but that .process is called 2x.  *Why?* So that the preprocessing defined by the training data is applied to the validation and optionally the test data later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = ils.label_from_df(dep_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(fastai.data_block.LabelLists, fastai.data_block.LabelList, 2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ll), type(ll.train), len(ll.lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelList\n",
       "y: CategoryList (45000 items)\n",
       "[Category 4, Category 5, Category 3, Category 4, Category 2]...\n",
       "Path: data/yelp_dataset\n",
       "x: MixedTabularList (45000 items)\n",
       "[MixedTabularLine business_id 8jpIK1WHmzzbXPaK51GenQ; user_id g6gTSnUKZIxLZPQVrFKscw; business_stars 3.5; business_postal_code 85226; business_state AZ; useful -0.1049; user_average_stars 0.4985; user_review_count -0.2776; business_review_count -0.3911; Text: xxbos i 've been here at least 10 times ... i like it ... but its not my favorite . i always get the spanish omelette egg whites without chorizo because i do n't eat meat . xxmaj once the chef forgot and put it in and i could hear him swearing from my table when he had to remake it . xxmaj the wait staff are all very friendly , but seem a bit overwhelmed keeping up with drink refills . xxmaj once my omelette came out scrambled instead of an omelette because the chef says its too hard to make an omelette out of egg whites ... which i 've gotten the other 9 times . xxmaj they are a mom and pop restaurant and i sometimes think they feel like it 's ok to do it there way rather then the customers way . \n",
       "\n",
       " xxmaj all in all though , with the exception of those 2 times , the food has always been great - they grill their english muffins on the griddle and let me tell you - those are awesome !, MixedTabularLine business_id wH4Q0y8C-lkq21yf4WWedw; user_id LAEJWZSvzsfWJ686VOaQig; business_stars 4.0; business_postal_code 85260; business_state AZ; useful 0.1975; user_average_stars 1.5758; user_review_count -0.3414; business_review_count -0.4012; Text: xxbos xxmaj although i had heard of xxmaj nekter , mainly from seeing it tagged in health conscious friends ' xxup ig posts , i had never tried it . xxmaj this location is rather new and conveniently located to me , so i gave it a try . xxmaj it 's xxup amazing ! xxmaj staff and the customer service they provide are phenomenal , and the having tried the juices ( fresh cold - pressed to order ) , smoothies & acai bowls , i 'm in love . xxmaj my 3 year old daughter even enjoyed the xxmaj pink xxmaj flamingo smoothie ( which they sell in kids size for little bellies ) . xxmaj they also sell pre - made detox and protein drinks in a small ready - to - go cooler area by the check out , although i have n't tried these yet . xxmaj and there 's a limited selection of granola & protein bars plus some kale chips in terms of snacks . \n",
       "\n",
       " xxmaj if you 're craving a treat , treat the xxmaj acai xxmaj berry or xxup pb xxmaj bowl which is like sorbet almost and comes topped with fresh berries , banana & granola , making it a perfect breakfast or afternoon snack . xxmaj little less filling but equally delicious are the smoothies . xxmaj or get refreshed and energized with the juices without the hassle of cleaning your juicer at home ( cuz anyone who juices knows it 's a pain in who - know - where ) ! xxmaj drinks and bowls run around $ 5 - 7 ish and there 's options to add protein , extra greens , or customize anything . \n",
       "\n",
       " xxmaj for me , this place has been a great place to stop by with my daughter after preschool and / or a good way to ensure i do n't go grocery shopping at xxmaj sprouts ( same parking lot ) hungry :), MixedTabularLine business_id cRMC2eQ9CP6ivhEY8EdaGg; user_id TwilnpgwW43r9-O2AS4PDQ; business_stars 3.5; business_postal_code M4Y 2C5; business_state ON; useful -0.1049; user_average_stars -0.7542; user_review_count -0.0708; business_review_count -0.4330; Text: xxbos xxmaj last week i met up with a highschool friend for the first time after highschool graduation for burritos at xxmaj chino xxmaj locos : xxmaj dos xxmaj locos , the xxmaj church street venue . i 'd never been to this xxmaj mexican fast food joint before , but i heard great things about their burritos , which is the only thing they make and serve . i was glad to find a spotless and nicely lit dining area and knowledgeable and friendly burrito cooks / cashiers ( they 're the same person ) . \n",
       "\n",
       " xxmaj their menu is rather limited , but that 's not necessarily a bad thing because what they offer is actually quite good and there 's something for everyone , particularly for me , the vegan : they have 1 vegetarian burrito with cheese and sour cream , but they also have 1 vegan burrito that is sans dairy and meat - \" the juicy vegan . \" xxmaj the vegan burrito was so good ... priced at $ 7.99 before taxes , it 's actually the most expensive burrito on their menu ! ! ! xxmaj the burrito consists of pressed tofu , eggplant , shitake mushrooms , glass noodles , guacamole , tomatoes , edamame beans , red onions , black beans , green peppers , cilantro , chipotle sauce , rustic rice ... all wrapped in a whole wheat wrap . ( xxmaj you have the option between white and whole wheat ) . xxmaj the cooks told me that they made their own guacamole at the restaurant , and to me , anything made from scratch is a big bonus ! xxmaj one set back to my burrito experience was the hot sauce ... it was n't hot enough , even though i 'd asked for extra hot xxup -i xxup love spicy food . \n",
       "\n",
       " xxmaj you should check this place out if you like xxmaj asian food and xxmaj mexican food ... the xxmaj asian - xxmaj mexican influence on my burrito made it a crazy dining experience for my taste buds ... those xxmaj chino xxmaj locos xxunk ! ( xxmaj translation , xxmaj chino xxmaj locos = \" crazy xxmaj asians \" in xxmaj spanish ) ., MixedTabularLine business_id zunMkZ4U2eVojempQtLngg; user_id eITkQlKYsYqOBASP-QS0iQ; business_stars 2.5; business_postal_code 85308; business_state AZ; useful -0.1049; user_average_stars -0.0276; user_review_count -0.1468; business_review_count -0.4432; Text: xxbos a friend and i were driving by and decided to stop in and check out the place . xxmaj glad we did . xxmaj we sat at the bar and ordered beer and burgers . 3.00 xxmaj fosters , a friendly bartender who had our names down in 5 minutes flat , and tasty burgers ! xxmaj lots of tvs around and the xxmaj suns beat the xxmaj thunder , so it was a great time !, MixedTabularLine business_id bWucOPNoIjd8ECdiDyVq9Q; user_id Ck3-SikwEb0U9G7RKh-O_w; business_stars 4.5; business_postal_code 85225; business_state AZ; useful -0.4074; user_average_stars -0.7918; user_review_count -0.3110; business_review_count -0.0069; Text: xxbos xxmaj it was n't bad xxrep 4 . but . we have been going to xxmaj yao for several years and it 's only 2 miles away from xxmaj singing xxmaj panda . xxmaj every time i find a new xxmaj chinese place , i am disappointed ... in comparison to xxmaj yao . xxmaj singing xxmaj panda was the same feeling . xxmaj plus , we spent 30 % more money for 30 % less food than we get at xxmaj yao xxup and it was n't nearly as tasty as xxmaj yao .]...\n",
       "Path: data/yelp_dataset"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MixedTabularLine business_id 8jpIK1WHmzzbXPaK51GenQ; user_id g6gTSnUKZIxLZPQVrFKscw; business_stars 3.5; business_postal_code 85226; business_state AZ; useful -0.1049; user_average_stars 0.4985; user_review_count -0.2776; business_review_count -0.3911; Text: xxbos i 've been here at least 10 times ... i like it ... but its not my favorite . i always get the spanish omelette egg whites without chorizo because i do n't eat meat . xxmaj once the chef forgot and put it in and i could hear him swearing from my table when he had to remake it . xxmaj the wait staff are all very friendly , but seem a bit overwhelmed keeping up with drink refills . xxmaj once my omelette came out scrambled instead of an omelette because the chef says its too hard to make an omelette out of egg whites ... which i 've gotten the other 9 times . xxmaj they are a mom and pop restaurant and i sometimes think they feel like it 's ok to do it there way rather then the customers way . \n",
       " \n",
       "  xxmaj all in all though , with the exception of those 2 times , the food has always been great - they grill their english muffins on the griddle and let me tell you - those are awesome !,\n",
       " Category 4,\n",
       " array([ 4126, 27899,     6,   380,     3]),\n",
       " ['business_id',\n",
       "  'user_id',\n",
       "  'business_stars',\n",
       "  'business_postal_code',\n",
       "  'business_state'],\n",
       " array([  2,  12,  99,  86, ..., 360,  39, 225,  19]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.train.x[0], ll.train.y[0], ll.train.x.codes[0], ll.train.x.cat_names, ll.train.x.text_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25217, 25217)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ll.train.x.vocab.itos), len(ll.valid.x.vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6: Build your DataBunch**\n",
    "\n",
    "We're skilling steps 4 (add a test dataset) and 5 (apply data augmentation) since we have neither a test set or any transforms we need to apply to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 64, 64, 64, torch.Size([64]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bunch = ll.databunch(bs=64)\n",
    "b = data_bunch.one_batch()\n",
    "len(b), len(b[0]), len(b[0][0]), len(b[0][1]), len(b[0][1]), b[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`len(b) = 2`:  the inputs and the targets\n",
    "\n",
    "`len(b[0]) = 3`: the three things in the input (cats, conts, text_ids)\n",
    "\n",
    "`len(b[0][0|1|2|]) = 64`: there are 64 of each of the 3 things (so there is a list 64 categorical tensors followed by a list of 64 continuous tensors that is followed by a list of 64 text tensors)\n",
    "\n",
    "The shape length of the categorical and continuous tensors are the same for every batch, whereas the shape of the numericalized token ids will be the same *per* batch thanks to the `mixed_tabular_pad_collate` function above.  This fulfills the requirement that each of the inputs be a squared matrix per batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([21069, 15870,     9,   498,    13]),\n",
       " tensor([ 0.1975, -1.0674,  0.2514, -0.2308]),\n",
       " tensor([    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     2,     4,   328,    37,    12,   164,    14,    45,   800,\n",
       "            78,    29,    10,  4725,     9,   155,   902,    39,    49,   728,\n",
       "           933,  1591,    10,    25,    33,    13,   429,  1264,     8,     4,\n",
       "            30,    11,    48,    39,    13,   184,   267,   127,   237,    14,\n",
       "           684,   148,    21,     8,     4,     9,   118,   246,    18,     9,\n",
       "           188,   240,    12,   322,    12,    15,   148,    14,    13,   696,\n",
       "          6456,   240,    12,    66,    16,    25,    39,  1958,    73,    10,\n",
       "            12,   111,   170,     9,  1516,    18,     5,    82,     5,   549,\n",
       "            85,    30,     9,   188,    18,    34,    44,   240,    54,    12,\n",
       "            68,    32,    57,    13,   488,  2063,    17,     9,   205,    20,\n",
       "           456,    48,    18,    84,   151,    12,    65,   101,   147,    29,\n",
       "            43,    18,    58,     8,     4,   201,    11,     9,   341,    18,\n",
       "          9665,    28,    13,  8515,   867,    85,    12,   826,    82,   687,\n",
       "            85,     9,   201,   695,    18,    23,    12,    15,   198,    14,\n",
       "           519,    10,    45,  4157,    83,    18,    34,  2468,    30,    12,\n",
       "            91,   410,  9809,    14,   671,    96,    98,    25,    38,   728,\n",
       "            10,    12,    15,   412,   240,    71,    26,    66,    48,    26,\n",
       "            80,   684,    81,    12,   826,     8,     4,   171,    14,     9,\n",
       "            42,   240,    12,    35,   155,   430,   474,   286,    10,   744,\n",
       "            10,    13,   605,  2250,     8,     4,   846,    28,   671,    15,\n",
       "             0,     6,   117,     8,  2447,  6135,   744,  8686,    19,     4,\n",
       "            23,    15,    13,   719,   546,   222,   240,    10,    12,    80,\n",
       "           173,    16,    15,   243,    30,    54,    29,    18,     9,   631,\n",
       "            17,    29,  5839,    17,    42,    12,    65,    60,    14,    45,\n",
       "            21,    75,  2363,   243,   205,    10,  1050,   240,   276,   244,\n",
       "          9341,   304,   456,    10,    12,    88,   173,    20,   170,    23,\n",
       "            12,    33,   837,    29,  5839,    17,    42,   160,    30,    16,\n",
       "            18,   378,   133,   443,    85,    16,    18,    44,   240,    30,\n",
       "            34,  2439,   765,  1159,    44,    85,    16,   103,   174, 15600,\n",
       "            55,    78,  1775,   110,  1606,    85,    21,   159,  1110,    26,\n",
       "            39,   771,    20,    49,    44,    42,    10,    44,    42,  1777,\n",
       "           342,    14,   141,    12,    65,    62,    60,    96,    14,   211,\n",
       "            13,   151,    14,     0,    21,    23,  1085,    16,    36,  1857,\n",
       "          1781,  3152,    96,    85,    30,    54,   342,    18,    84,  9634,\n",
       "            10,    26,    57,    32,   591,    13,   215,    17,    13,  4256,\n",
       "           205,   139,    26,    80,    34,    45,   357,   240]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][0][0], b[0][1][0], b[0][2][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows the categorical, continuous, and token ids for the first item in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "TABULAR:<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <tr>\n",
       "    <th>business_id</th>\n",
       "    <th>user_id</th>\n",
       "    <th>business_stars</th>\n",
       "    <th>business_postal_code</th>\n",
       "    <th>business_state</th>\n",
       "    <th>useful</th>\n",
       "    <th>user_average_stars</th>\n",
       "    <th>user_review_count</th>\n",
       "    <th>business_review_count</th>\n",
       "    <th>target</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2NiBvT5zL272IRcxru_x9A</th>\n",
       "    <th>WVbzw3IPJ29PWOsI2iESSw</th>\n",
       "    <th>4.0</th>\n",
       "    <th>85054</th>\n",
       "    <th>AZ</th>\n",
       "    <th>-0.4074</th>\n",
       "    <th>0.9495</th>\n",
       "    <th>-0.3383</th>\n",
       "    <th>-0.1494</th>\n",
       "    <th>5</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>vo8rCTuhM19GhpY07VtXpw</th>\n",
       "    <th>WVVkGFSoatEZWU2oGdn4fQ</th>\n",
       "    <th>2.0</th>\n",
       "    <th>85016</th>\n",
       "    <th>AZ</th>\n",
       "    <th>-0.1049</th>\n",
       "    <th>-1.7689</th>\n",
       "    <th>-0.3475</th>\n",
       "    <th>-0.4712</th>\n",
       "    <th>1</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>wghDrzcZ0VloAtaIZ7GEBg</th>\n",
       "    <th>q9yxse9JxjhnEJy42leUGg</th>\n",
       "    <th>4.5</th>\n",
       "    <th>85016</th>\n",
       "    <th>AZ</th>\n",
       "    <th>-0.4074</th>\n",
       "    <th>0.4359</th>\n",
       "    <th>-0.3231</th>\n",
       "    <th>-0.2028</th>\n",
       "    <th>4</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3dw6xhzG08htY5HcL2OjeA</th>\n",
       "    <th>TzRbkwSLFym6pgPJKYT5xw</th>\n",
       "    <th>3.5</th>\n",
       "    <th>85212</th>\n",
       "    <th>AZ</th>\n",
       "    <th>-0.1049</th>\n",
       "    <th>0.5987</th>\n",
       "    <th>-0.2897</th>\n",
       "    <th>-0.2397</th>\n",
       "    <th>3</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>IWDA6Tp8aFIaIqJJnZF0oA</th>\n",
       "    <th>0k4ZY5M55ceFdbw74AS_kA</th>\n",
       "    <th>2.0</th>\n",
       "    <th>44107</th>\n",
       "    <th>OH</th>\n",
       "    <th>-0.1049</th>\n",
       "    <th>0.1728</th>\n",
       "    <th>-0.3323</th>\n",
       "    <th>-0.4572</th>\n",
       "    <th>2</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "TEXT:<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>  <col width='90%'>  <col width='1%'>  <tr>\n",
       "    <th>text_data</th>\n",
       "    <th>target</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxbos(2) xxmaj(4) service(59) from(70) xxmaj(4) coco(5707) was(15) top(304) notch(1248) !(19) xxmaj(4) they(25) know(147) their(69) food(42) and(10) wine(453) parings(22235) -(41) great(50) happy(236) hour(265) ,(11) decent(397) prices(244) for(20) fresh(192) food(42) prepared(767) perfectly(590) .(8) 10-minute(20013) from(70) the(9) xxmaj(4) marriott(5151) desert(1412) ridge(8270) .(8) xxmaj(4) nice(104) outdoor(1194) patio(596) .(8) xxmaj(4) would(65) definitely(131) go(76) again(134) .(8) xxmaj(4) great(50) adult(2402) venue(1476) .(8)</th>\n",
       "    <th>5</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxbos(2) xxmaj(4) this(29) place(43) will(80) kill(3152) your(87) dog(515) .(8) xxmaj(4) then(139) they(25) wo(390) n't(32) have(33) the(9) decency(9828) to(14) refund(1543) your(87) pet(1555) fees(1935) that(23) you(26) had(35) to(14) pay(352) the(9) entire(592) year(408) living(1234) there(48) .(8) xxmaj(4) if(54) you(26) have(33) a(13) pet(1555) ,(11) do(57) n't(32) live(513) here(58) .(8) xxmaj(4) maintenance(1727) will(80) show(387) up(73) while(181) you(26) are(39) at(40) work(187) unannounced(19606) .(8) xxmaj(4) your(87) dog(515) will(80) escape(3445) because(98) he(77) 's(36) confused(1983) and(10) scared(3007) .(8) xxmaj(4) the(9) apartment(1663) complex(2052) will(80) not(34) tell(354) you(26) why(298) they(25) are(39) calling(1391) you(26) at(40) work(187) until(432) you(26) show(387) up(73) at(40) 5(142) pm(551) and(10) realize(1487) your(87) dog(515) is(18) missing(1268) .(8) xxmaj(4) they(25) will(80) cover(1459) their(69) own(450) a(13) *(475) *(475) before(160) attempting(5914) to(14) find(211) your(87) dog(515) .(8) xxmaj(4) the(9) next(201) day(154) you(26) will(80) find(211) your(87) dog(515) dead(1927) on(31) the(9) side(242) of(17) 16th(7260) street(532) in(21) front(343) of(17) the(9) xxmaj(4) salvation(17185) xxmaj(4) army(9624) .(8) xxmaj(4) you(26) will(80) be(45) heartbroken(23334) and(10) they(25) will(80) be(45) unforgivable(20204) during(368) the(9) process(889) .(8) xxmaj(4) do(57) n't(32) live(513) here(58)</th>\n",
       "    <th>1</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxbos(2) \"(56) xxmaj(4) interesting(861) \"(56) because(98) it(16) starts(2253) out(55) pretty(149) tame(12263) ,(11) but(30) if(54) you(26) 're(162) going(148) to(14) the(9) top(304) of(17) the(9) trail(2748) ,(11) you(26) 're(162) in(21) for(20) a(13) hike(3365) !(19) xxmaj(4) you(26) 'll(212) be(45) xxunk(0) at(40) the(9) top(304) !(19) !(19)</th>\n",
       "    <th>4</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxbos(2) xxmaj(4) once(303) again(134) the(9) food(42) made(137) up(73) for(20) the(9) bad(206) customer(210) service(59) .(8) xxmaj(4) how(140) are(39) you(26) supposed(1119) to(14) eat(191) snow(2440) crab(654) without(346) a(13) way(151) to(14) get(66) it(16) out(55) of(17) the(9) shell(2057) ?(100) xxmaj(4) we(24) skipped(4664) the(9) crab(654) this(29) time(63) because(98) the(9) waitress(377) said(152) the(9) \"(56) health(1462) dept(6129) .(8) \"(56) wo(390) n't(32) allow(1978) them(96) to(14) use(339) the(9) scissors(8965) anymore(1401) but(30) you(26) could(103) use(339) a(13) plastic(1830) fork(3002) to(14) process(889) your(87) crab(654) legs(1747) ...(85) later(399) we(24) were(38) told(186) by(94) a(13) more(93) helpful(384) waiter(537) that(23) they(25) were(38) just(62) out(55) of(17) them(96) because(98) it(16) 's(36) busy(331) ((53) xxmaj(4) fri(6065) night(175) )(51) understandable(3671) but(30) not(34) acceptable(2873) xxmaj(4) so(37) a(13) lie(2518) /(110) incompetence(10430) in(21) the(9) back(72) cost(631) the(9) restaurant(125) 36(4795) bucks(1267) as(46) three(422) of(17) our(61) party(478) got(97) meals(879) 1(274) /(110) 2(150) of(17) what(81) the(9) crab(654) cost(631) and(10) the(9) second(428) strike(4745) of(17) not(34) coming(290) back(72) .(8) xxup(5) and(10) another(199) big(250) fail(3117) was(15) that(23) one(67) meal(222) came(114) out(55) nice(104) and(10) quick(371) ((53) the(9) shrimp(430) )(51) but(30) the(9) other(102) three(422) in(21) the(9) party(478) were(38) a(13) full(300) 25(948) minutes(177) later(399) xxrep(6) 4(117) .(8) and(10) it(16) was(15) our(61) fault(1716) the(9) fryer(6066) was(15) backed(4267) up(73) apparently(1223) ...(85) xxmaj(4) waiting(394) a(13) couple(379) of(17) weeks(675) but(30) xxmaj(4) three(422) strikes(8083) and(10) you(26) 're(162) out(55) xxrep(6) 4(117) .(8)</th>\n",
       "    <th>3</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxbos(2) this(29) location(188) is(18) always(116) late(538) on(31) orders(682) .(8) i(12) placed(1328) an(75) order(119) and(10) was(15) to(14) 15(445) -(41) 20(402) minutes(177) .(8) i(12) waited(479) until(432) 20(402) minutes(177) until(432) i(12) left(275) my(22) house(334) ,(11) to(14) give(190) them(96) the(9) benefit(3646) of(17) a(13) couple(379) extra(418) minutes(177) .(8) get(66) there(48) ,(11) pizza(195) was(15) n't(32) even(101) started(439) yet(496) .(8) did(68) n't(32) remember(701) my(22) order(119) i(12) called(273) in(21) .(8) ended(558) up(73) waiting(394) for(20) 30(495) at(40) the(9) restaurant(125) for(20) them(96) to(14) make(141) the(9) 1(274) pizza(195) .(8) xxmaj(4) all(52) in(21) all(52) ,(11) it(16) took(183) over(130) an(75) hour(265) to(14) munch(6693) on(31) some(90) pizza(195) .(8) when(71) it(16) should(234) have(33) only(92) taken(708) 20(402) .(8) xxmaj(4) they(25) get(66) two(155) starts(2253) because(98) the(9) pizza(195) was(15) actually(299) better(153) than(121) expected(698) .(8)</th>\n",
       "    <th>2</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_bunch.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we included the `Normalize` proc, notice that the continuous variables are normalized *per dataset*: \n",
    "`(x - x.mean) / x.std`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
